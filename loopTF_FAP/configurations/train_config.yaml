batch_size: 128
lr_scheduler: cosine
cosine_scheduler_config:
  lr_decay_steps: 300000.0
  min_lr: 1.0e-05
  warmup_steps: 1000
StepLR_scheduler_config:
  gamma: 0.1
  step_size: 1000
learning_rate: 0.001
max_epochs: 500
optimizer: AdamW
wandb_config:
  wandb_entity: transformer-computation-graph
  wandb_project: loopTF-FAP-v2
weight_decay: 0.008
seed: null
num_workers: 16

max_oper: 1000
max_dep: 6
med_loss_ratio:
  - 1.0
  - 1.0
  - 1.0
  - 1.0
  - 1.0
  - 1.0
use_loss_n: False
loss_n_scale: 0.0
add_input_per_loop: false
use_parent_loss: True
loss_parent_scale: 0.5
log_mrr: true
log_acc: false
loss_eq_scale: 1.0